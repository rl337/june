"""
Triton model repository setup command - Create and validate Triton Inference Server model repository structure.

Usage:
    poetry run -m essence setup-triton-repository --model qwen3-30b --version 1
    poetry run -m essence setup-triton-repository --validate --model qwen3-30b
    poetry run -m essence setup-triton-repository --list

This command helps set up the Triton Inference Server model repository structure
required for TensorRT-LLM models. It creates the directory structure and provides
guidance on what files need to be placed in each model directory.

For Phase 15 Task 4: Model repository setup for TensorRT-LLM.
"""
import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from essence.command import Command

logger = logging.getLogger(__name__)


class TritonRepositoryManager:
    """
    Manager for Triton Inference Server model repository structure.
    
    Creates and validates the directory structure required for TensorRT-LLM models
    in Triton Inference Server.
    """
    
    def __init__(self, repository_path: str = "/home/rlee/models/triton-repository"):
        """
        Initialize repository manager.
        
        Args:
            repository_path: Path to Triton model repository root directory
        """
        self.repository_path = Path(repository_path)
    
    def create_model_structure(self, model_name: str, version: str = "1") -> Tuple[bool, str]:
        """
        Create directory structure for a model.
        
        Args:
            model_name: Name of the model (e.g., "qwen3-30b")
            version: Model version (default: "1")
        
        Returns:
            Tuple of (success: bool, message: str)
        """
        try:
            model_dir = self.repository_path / model_name / version
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # Create a README with instructions
            readme_path = model_dir / "README.md"
            if not readme_path.exists():
                readme_content = f"""# Model: {model_name} (version {version})

This directory should contain the following files for TensorRT-LLM:

## Required Files:

1. **config.pbtxt** - Triton model configuration file
   - Defines model inputs/outputs, instance groups, and TensorRT-LLM backend settings
   - Example structure:
     ```
     name: "{model_name}"
     platform: "tensorrt_llm"
     max_batch_size: 0
     input [
       {{
         name: "text_input"
         data_type: TYPE_STRING
         dims: [ -1 ]
       }}
     ]
     output [
       {{
         name: "text_output"
         data_type: TYPE_STRING
         dims: [ -1 ]
       }}
     ]
     instance_group [
       {{
         count: 1
         kind: KIND_GPU
       }}
     ]
     ```

2. **TensorRT-LLM Engine Files** - Compiled model engine files
   - Typically includes: `*.engine` files, weights, and other artifacts
   - Generated by TensorRT-LLM build tools

3. **Tokenizer Files** - Tokenizer configuration and vocabulary
   - `tokenizer.json` or `tokenizer_config.json`
   - `vocab.json` or similar vocabulary files
   - Usually copied from HuggingFace model directory

## Model Compilation:

Before placing files here, the model must be compiled using TensorRT-LLM build tools.
See REFACTOR_PLAN.md Phase 15 Task 4 for compilation instructions.

## Model Loading:

Once files are in place, load the model using:
```bash
poetry run -m essence manage-tensorrt-llm --action load --model {model_name}
```

## Verification:

Check model status:
```bash
poetry run -m essence manage-tensorrt-llm --action status --model {model_name}
```
"""
                readme_path.write_text(readme_content)
                logger.info(f"Created README.md in {model_dir}")
            
            logger.info(f"Created model repository structure: {model_dir}")
            return True, f"Created model repository structure for '{model_name}' version {version} at {model_dir}"
            
        except Exception as e:
            error_msg = f"Failed to create model repository structure: {e}"
            logger.error(error_msg, exc_info=True)
            return False, error_msg
    
    def validate_model_structure(self, model_name: str, version: str = "1") -> Tuple[bool, List[str], str]:
        """
        Validate that a model directory has the required structure and files.
        
        Args:
            model_name: Name of the model to validate
            version: Model version to validate
        
        Returns:
            Tuple of (is_valid: bool, missing_files: List[str], message: str)
        """
        model_dir = self.repository_path / model_name / version
        
        if not model_dir.exists():
            return False, [], f"Model directory does not exist: {model_dir}"
        
        missing_files = []
        warnings = []
        
        # Check for config.pbtxt
        config_file = model_dir / "config.pbtxt"
        if not config_file.exists():
            missing_files.append("config.pbtxt")
        else:
            logger.info(f"Found config.pbtxt: {config_file}")
        
        # Check for engine files (common patterns)
        engine_files = list(model_dir.glob("*.engine"))
        if not engine_files:
            warnings.append("No .engine files found (TensorRT-LLM engine files)")
        else:
            logger.info(f"Found {len(engine_files)} engine file(s)")
        
        # Check for tokenizer files
        tokenizer_files = list(model_dir.glob("tokenizer*.json")) + list(model_dir.glob("vocab*.json"))
        if not tokenizer_files:
            warnings.append("No tokenizer files found (tokenizer.json, vocab.json, etc.)")
        else:
            logger.info(f"Found {len(tokenizer_files)} tokenizer file(s)")
        
        if missing_files:
            return False, missing_files, f"Model structure incomplete. Missing: {', '.join(missing_files)}"
        elif warnings:
            return True, [], f"Model structure exists but has warnings: {'; '.join(warnings)}"
        else:
            return True, [], f"Model structure is valid: {model_dir}"
    
    def list_models(self) -> Tuple[bool, List[Dict[str, any]], str]:
        """
        List all models in the repository.
        
        Returns:
            Tuple of (success: bool, models: List[Dict], message: str)
            Each model dict contains: name, versions, path
        """
        if not self.repository_path.exists():
            return False, [], f"Repository directory does not exist: {self.repository_path}"
        
        models = []
        try:
            for model_dir in self.repository_path.iterdir():
                if model_dir.is_dir():
                    versions = []
                    for version_dir in model_dir.iterdir():
                        if version_dir.is_dir() and version_dir.name.isdigit():
                            versions.append(version_dir.name)
                    
                    if versions:
                        models.append({
                            "name": model_dir.name,
                            "versions": sorted(versions, key=int),
                            "path": str(model_dir)
                        })
            
            models.sort(key=lambda x: x["name"])
            return True, models, f"Found {len(models)} model(s) in repository"
            
        except Exception as e:
            error_msg = f"Error listing models: {e}"
            logger.error(error_msg, exc_info=True)
            return False, [], error_msg


class SetupTritonRepositoryCommand(Command):
    """
    Command for setting up and validating Triton Inference Server model repository structure.
    
    Helps create the directory structure required for TensorRT-LLM models and validates
    that models have the required files before loading.
    
    This supports Phase 15 Task 4: Model repository setup.
    """
    
    @classmethod
    def get_name(cls) -> str:
        """
        Get the command name.
        
        Returns:
            Command name: "setup-triton-repository"
        """
        return "setup-triton-repository"
    
    @classmethod
    def get_description(cls) -> str:
        """
        Get the command description.
        
        Returns:
            Description of what this command does
        """
        return "Setup and validate Triton Inference Server model repository structure"
    
    @classmethod
    def add_args(cls, parser: argparse.ArgumentParser) -> None:
        """
        Add command-line arguments to the argument parser.
        
        Args:
            parser: Argument parser to add arguments to
        """
        parser.add_argument(
            "--action", "-a",
            choices=["create", "validate", "list"],
            default="create",
            help="Action to perform: create, validate, or list (default: create)"
        )
        parser.add_argument(
            "--model", "-m",
            type=str,
            help="Model name (required for create/validate actions)"
        )
        parser.add_argument(
            "--version", "-v",
            type=str,
            default="1",
            help="Model version (default: 1)"
        )
        parser.add_argument(
            "--repository-path",
            type=str,
            default=os.getenv("TENSORRT_LLM_MODEL_REPOSITORY", "/home/rlee/models/triton-repository"),
            help="Path to Triton model repository root (default: /home/rlee/models/triton-repository)"
        )
    
    def init(self) -> None:
        """
        Initialize the command.
        
        Validates arguments and initializes repository manager.
        """
        # Validate arguments
        if self.args.action in ["create", "validate"] and not self.args.model:
            logger.error(f"Model name is required for action '{self.args.action}'")
            sys.exit(1)
        
        # Initialize manager
        self.manager = TritonRepositoryManager(
            repository_path=self.args.repository_path
        )
        logger.info(f"Initialized Triton repository manager (path: {self.args.repository_path})")
    
    def run(self) -> None:
        """
        Execute the requested action.
        
        Performs create/validate/list operation based on command arguments.
        """
        action = self.args.action
        
        if action == "create":
            success, message = self.manager.create_model_structure(
                self.args.model,
                self.args.version
            )
            if success:
                print(f"‚úÖ {message}")
                print(f"\nüìÅ Model directory created at:")
                print(f"   {self.args.repository_path}/{self.args.model}/{self.args.version}")
                print(f"\nüìù Next steps:")
                print(f"   1. Compile the model using TensorRT-LLM build tools")
                print(f"   2. Place compiled files in the model directory")
                print(f"   3. Create config.pbtxt with model configuration")
                print(f"   4. Load the model: poetry run -m essence manage-tensorrt-llm --action load --model {self.args.model}")
                sys.exit(0)
            else:
                print(f"‚ùå {message}")
                sys.exit(1)
        
        elif action == "validate":
            is_valid, missing_files, message = self.manager.validate_model_structure(
                self.args.model,
                self.args.version
            )
            if is_valid:
                if missing_files:
                    print(f"‚ö†Ô∏è  {message}")
                else:
                    print(f"‚úÖ {message}")
                sys.exit(0)
            else:
                print(f"‚ùå {message}")
                if missing_files:
                    print(f"\nMissing files:")
                    for file in missing_files:
                        print(f"  - {file}")
                sys.exit(1)
        
        elif action == "list":
            success, models, message = self.manager.list_models()
            if success:
                print(f"\nüìã {message}\n")
                if models:
                    print("Models in repository:")
                    for model in models:
                        name = model["name"]
                        versions = ", ".join(model["versions"])
                        print(f"  ‚Ä¢ {name} (versions: {versions})")
                else:
                    print("  (no models found)")
                    print(f"\nüí° Create a model structure:")
                    print(f"   poetry run -m essence setup-triton-repository --action create --model <name>")
                sys.exit(0)
            else:
                print(f"‚ùå {message}")
                sys.exit(1)
    
    def cleanup(self) -> None:
        """
        Clean up resources.
        
        No cleanup needed for this command.
        """
        pass
