# AGENTS.md - Development Guide for June Agent

This document provides essential information for AI agents working on the June Agent project, including architecture details, development practices, and environment specifics.

## üèóÔ∏è Architecture Overview

June Agent is a microservices-based interactive autonomous agent system optimized for NVIDIA DGX Spark with the following architecture:

### Core Services
- **Gateway Service** (Port 8000) - FastAPI + WebSocket ingress with auth, rate limiting
- **Inference API** (Port 50051) - gRPC coordinator for LLM orchestration with RAG
- **STT Service** (Port 50052) - Speech-to-Text with Whisper, VAD, gRPC streaming
- **TTS Service** (Port 50053) - Text-to-Speech with FastSpeech2/HiFi-GAN streaming
- **Webapp Service** (Port 3000) - React-based Telegram-like chat interface

### Supporting Infrastructure
- **PostgreSQL + pgvector** (Port 5432) - RAG storage and conversation memory
- **MinIO** (Ports 9000/9001) - S3-compatible object storage
- **NATS** (Port 4222) - Pub/sub messaging
- **Prometheus** (Port 9090) - Metrics collection
- **Grafana** (Port 3000) - Dashboards
- **Loki** (Port 3100) - Log aggregation
- **Jaeger** (Port 16686) - Distributed tracing

## ü§ñ Current Model Configuration

**Primary LLM:** `Qwen/Qwen3-30B-A3B-Thinking-2507`
- **Context Window:** 128k tokens with Yarn expansion
- **Device:** CUDA GPU 0 with MPS sharing
- **Quantization:** 4-bit for memory efficiency
- **Capabilities:** Advanced reasoning, tool use, RAG integration

**STT Model:** `openai/whisper-large-v3`
- **Features:** Voice Activity Detection (VAD), real-time streaming
- **Languages:** Multi-language support

**TTS Model:** `facebook/fastspeech2-en-ljspeech`
- **Features:** Multiple voices, prosody control, streaming output

## üö® CRITICAL: Model Cache Management

**STRICT POLICY:** Models MUST be downloaded using the authorized download script ONLY.

### Model Cache Directory
- **Location:** `/home/rlee/models`
- **Structure:**
  - `/home/rlee/models/huggingface/` - Hugging Face models
  - `/home/rlee/models/transformers/` - Transformers cache
  - `/home/rlee/models/whisper/` - Whisper models
  - `/home/rlee/models/tts/` - TTS models

### Authorized Model Download
**ONLY** use `scripts/download_models.py` for model downloads:

```bash
# Download all required models
python scripts/download_models.py --all

# Download specific model
python scripts/download_models.py --model Qwen/Qwen3-30B-A3B-Thinking-2507

# Check cache status
python scripts/download_models.py --status

# List authorized models
python scripts/download_models.py --list
```

### Runtime Model Loading
- **Services MUST use local cache only**
- **NO internet downloads during runtime**
- **Set environment variables:**
  - `MODEL_CACHE_DIR=/home/rlee/models`
  - `HUGGINGFACE_CACHE_DIR=/home/rlee/models/huggingface`
  - `TRANSFORMERS_CACHE_DIR=/home/rlee/models/transformers`

### Security Rules
1. **NEVER** allow services to download models automatically
2. **ALWAYS** use `local_files_only=True` in model loading
3. **VERIFY** models exist in cache before starting services
4. **AUDIT** model cache directory regularly

## üìÅ Data Directory Structure

**Primary Data Directory:** `/home/rlee/june_data`
- **PostgreSQL:** `/home/rlee/june_data/postgres/` - Database files
- **MinIO:** `/home/rlee/june_data/minio/` - Object storage
- **NATS:** `/home/rlee/june_data/nats/data/` - Message broker data
- **NATS JetStream:** `/home/rlee/june_data/nats/jets_stream/` - Stream storage
- **Prometheus:** `/home/rlee/june_data/prometheus/` - Metrics data
- **Grafana:** `/home/rlee/june_data/grafana/` - Dashboard configs
- **Loki:** `/home/rlee/june_data/loki/` - Log aggregation
- **Logs:** `/home/rlee/june_data/logs/` - Application logs
- **Uploads:** `/home/rlee/june_data/uploads/` - User uploads
- **Backups:** `/home/rlee/june_data/backups/` - System backups

**Environment Variable:** `JUNE_DATA_DIR=/home/rlee/june_data`

**Important:** This directory will grow very large over time and is excluded from git.

## üê≥ Container Environment

### Docker-First Development Strategy
**CRITICAL:** All development tools and CLI utilities MUST run in Docker containers.

**Why Docker-First?**
- **Consistency:** Same environment across all developers
- **Dependency Isolation:** No conflicts with host system libraries
- **Reproducibility:** Guaranteed working environment
- **Security:** Isolated execution environment
- **Version Control:** Exact dependency versions locked

### CLI Tools Container
**Service:** `cli-tools` (Profile: `tools`)
- **Purpose:** All command-line tools and utilities
- **Base Image:** `python:3.11-slim`
- **Dependencies:** ML libraries, development tools, audio processing
- **Access:** `docker exec -it june-cli-tools bash`

**Available Tools:**
- Model download script (`scripts/download_models.py`)
- Development utilities (black, isort, flake8, mypy)
- Testing tools (pytest, pytest-cov)
- Audio processing (whisper, TTS, librosa)

### GPU Configuration
- **Single GPU Sharing:** All services share GPU 0 via CUDA MPS
- **Memory Management:** Paged KV cache, model quantization
- **CUDA Environment Variables:**
  - `CUDA_VISIBLE_DEVICES=0`
  - `CUDA_MPS_ENABLE_PER_CTX_SM_PARTITIONING=1`

### Docker Compose Services
All services are orchestrated via `docker-compose.yml` with:
- Health checks for all services
- Volume mounts for model cache and data persistence
- Network isolation with `june_network`
- Resource limits and GPU allocation

## üìÅ Project Structure

```
dev/june/
‚îú‚îÄ‚îÄ proto/                    # gRPC protobuf definitions
‚îÇ   ‚îú‚îÄ‚îÄ asr.proto            # Speech-to-Text service
‚îÇ   ‚îú‚îÄ‚îÄ tts.proto            # Text-to-Speech service
‚îÇ   ‚îî‚îÄ‚îÄ llm.proto            # LLM inference service
‚îú‚îÄ‚îÄ services/                 # Microservices
‚îÇ   ‚îú‚îÄ‚îÄ gateway/             # FastAPI + WebSocket gateway
‚îÇ   ‚îú‚îÄ‚îÄ inference-api/       # LLM orchestration
‚îÇ   ‚îú‚îÄ‚îÄ stt/                 # Speech-to-Text
‚îÇ   ‚îú‚îÄ‚îÄ tts/                 # Text-to-Speech
‚îÇ   ‚îî‚îÄ‚îÄ webapp/              # React chat interface
‚îú‚îÄ‚îÄ shared/                   # Common utilities
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # Shared utilities
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ config/                   # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ postgres-init.sql     # Database schema
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml        # Metrics config
‚îÇ   ‚îú‚îÄ‚îÄ loki-config.yml       # Logging config
‚îÇ   ‚îî‚îÄ‚îÄ grafana/              # Dashboard configs
‚îú‚îÄ‚îÄ tests/integration/        # System integration tests
‚îú‚îÄ‚îÄ docker-compose.yml        # Service orchestration
‚îú‚îÄ‚îÄ .env.example             # Environment template
‚îú‚îÄ‚îÄ pyproject.toml           # Python dependencies
‚îú‚îÄ‚îÄ README.md                # User documentation
‚îî‚îÄ‚îÄ AGENTS.md                # This file
```

## üîß Development Practices

### Code Quality Standards
- **Python:** Black formatting, isort imports, flake8 linting, mypy type checking
- **TypeScript/React:** ESLint, Prettier formatting
- **Testing:** Comprehensive test suites for all services
- **Documentation:** Inline docstrings, README updates

### Testing Strategy
Each service includes:
- **Unit Tests:** Individual component testing
- **Integration Tests:** Service interaction testing
- **Mock Tests:** External dependency isolation
- **Performance Tests:** Concurrent request handling
- **Error Handling Tests:** Failure scenario coverage

### Service Communication
- **Internal:** gRPC for service-to-service communication
- **External:** REST API and WebSocket for client access
- **Messaging:** NATS for pub/sub events
- **Storage:** PostgreSQL for structured data, MinIO for objects

## üöÄ Deployment Commands

### Start Full System
```bash
cd dev/june
cp .env.example .env
# Edit .env with your configuration
docker-compose up -d
```

### Individual Service Development
```bash
# Build specific service
cd services/gateway
docker build -t june-gateway .

# Run with live reload
docker run -p 8000:8000 -v $(pwd):/app june-gateway
```

### Health Checks
```bash
# Run comprehensive health checks
./run_checks.sh

# Check specific service
curl http://localhost:8000/health  # Gateway
curl http://localhost:50051/health # Inference API (gRPC)
```

## üîç Monitoring and Debugging

### Metrics Endpoints
- Gateway: `http://localhost:8000/metrics`
- Inference API: `http://localhost:8001/metrics`
- STT: `http://localhost:8002/metrics`
- TTS: `http://localhost:8003/metrics`

### Dashboards
- Grafana: `http://localhost:3000` (admin/admin)
- Prometheus: `http://localhost:9090`
- Jaeger: `http://localhost:16686`

### Logs
```bash
# View all logs
docker-compose logs -f

# View specific service logs
docker-compose logs -f gateway
docker-compose logs -f inference-api
```

## üõ†Ô∏è Common Development Tasks

### Using CLI Tools Container
**Start CLI Tools:**
```bash
# Start CLI tools container
docker-compose --profile tools up -d cli-tools

# Access CLI tools
docker exec -it june-cli-tools bash
```

**Model Management:**
```bash
# Download all models
docker exec -it june-cli-tools python scripts/download_models.py --all

# Download specific model
docker exec -it june-cli-tools python scripts/download_models.py --model Qwen/Qwen3-30B-A3B-Thinking-2507

# Check model status
docker exec -it june-cli-tools python scripts/download_models.py --status

# List authorized models
docker exec -it june-cli-tools python scripts/download_models.py --list
```

**Development Tools:**
```bash
# Code formatting
docker exec -it june-cli-tools black /app/scripts/
docker exec -it june-cli-tools isort /app/scripts/

# Linting
docker exec -it june-cli-tools flake8 /app/scripts/
docker exec -it june-cli-tools mypy /app/scripts/

# Testing
docker exec -it june-cli-tools pytest /app/scripts/
```

### Adding New CLI Tools
1. **Add dependencies** to `services/cli-tools/requirements-cli.txt`
2. **Create tool script** in `services/cli-tools/scripts/`
3. **Update Dockerfile** if system dependencies needed
4. **Test tool** in CLI container
5. **Update documentation** with usage instructions

### Adding New Features
1. **Update protobuf schemas** if needed
2. **Implement service logic** with comprehensive tests
3. **Update docker-compose.yml** for new services
4. **Add health checks** and metrics
5. **Update documentation**

### Adding New Models
1. **Add to AUTHORIZED_MODELS** in `scripts/download_models.py`
2. **Update download script** with new model category
3. **Test model download** using the script
4. **Update service code** to use local cache only
5. **Verify no internet access** during runtime
6. **Update documentation** with new model info

### Debugging Issues
1. **Check service health** with `./run_checks.sh`
2. **Review logs** for error messages
3. **Verify GPU allocation** with `nvidia-smi`
4. **Test individual services** in isolation
5. **Check network connectivity** between services

### Performance Optimization
1. **Monitor GPU memory usage** with `nvidia-smi`
2. **Check Prometheus metrics** for bottlenecks
3. **Profile service performance** with timing logs
4. **Optimize model loading** and inference
5. **Scale services horizontally** if needed

## üîê Security Considerations

### Authentication
- JWT tokens for API access
- Rate limiting to prevent abuse
- Input validation and sanitization

### Network Security
- Internal service communication over gRPC
- External access through Gateway only
- CORS configuration for webapp

### Data Protection
- Environment variables for secrets
- Secure storage of audio/text data
- User session management

## üìä Performance Characteristics

### Expected Performance
- **Text Generation:** ~50-100 tokens/second
- **Speech Recognition:** ~2-3x real-time
- **Text-to-Speech:** ~1-2x real-time
- **Memory Usage:** ~20-30GB GPU memory
- **Latency:** <500ms for simple requests

### Scaling Considerations
- **Horizontal:** Stateless services can be scaled
- **Vertical:** GPU memory limits model size
- **Database:** PostgreSQL can be sharded
- **Storage:** MinIO can be clustered

## üéØ Future Development Areas

### Planned Features
- **Multi-language Support:** Language detection and translation
- **Advanced RAG:** Document ingestion and retrieval
- **Custom Voice Training:** User-specific voice models
- **Tool Integration:** External API connections
- **Fine-tuning Interface:** Model customization UI

### Architecture Improvements
- **Kubernetes Deployment:** Production orchestration
- **Service Mesh:** Advanced networking
- **Caching Layer:** Redis for performance
- **Load Balancing:** Multiple gateway instances

## üö® Troubleshooting Guide

### Common Issues

**GPU Memory Errors:**
- Check `nvidia-smi` for memory usage
- Reduce model quantization or context length
- Restart services to clear memory

**Service Connection Errors:**
- Verify docker-compose network configuration
- Check service health endpoints
- Review NATS connectivity

**Model Loading Failures:**
- Verify Hugging Face token
- Check internet connectivity
- Clear model cache if corrupted

**WebSocket Connection Issues:**
- Check Gateway service status
- Verify authentication tokens
- Review browser console for errors

### Recovery Procedures
1. **Full System Restart:** `docker-compose down && docker-compose up -d`
2. **Service-Specific Restart:** `docker-compose restart <service>`
3. **Data Reset:** Remove volumes and restart
4. **Model Cache Clear:** Remove `~/.cache/huggingface` volume

## üìù Development Notes

### Current Limitations
- Single GPU deployment only
- Limited to English language models
- Basic tool integration
- No persistent user sessions

### Technical Debt
- WebSocket authentication needs improvement
- Error handling could be more granular
- Metrics collection needs expansion
- Test coverage could be higher

### Known Issues
- Occasional GPU memory fragmentation
- WebSocket reconnection handling
- Audio format compatibility
- Model loading race conditions

---

**Last Updated:** December 2024  
**Version:** 0.2.0  
**Maintainer:** June Agent Team
