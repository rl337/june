services:
  base:
    build:
      context: .
      dockerfile: ./services/base/Dockerfile
    image: june-base:latest
    profiles:
    - tools
    volumes:
    - /var/log/june/base:/logs
  cli-tools:
    build:
      context: .
      dockerfile: ./services/cli-tools/Dockerfile
    container_name: june-cli-tools
    environment:
    - MODEL_CACHE_DIR=/models
    - HUGGINGFACE_CACHE_DIR=/models/huggingface
    - TRANSFORMERS_CACHE_DIR=/models/transformers
    - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    - MODEL_NAME=${MODEL_NAME:-Qwen/Qwen3-30B-A3B-Thinking-2507}
    volumes:
    - ${MODEL_CACHE_DIR:-/home/rlee/models}:/models
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}:/data
    - ${JUNE_TEST_DATA_DIR:-/home/rlee/june_test_data}:/test_data
    - ./services/cli-tools/scripts:/app/scripts
    - ./scripts:/app/root_scripts
    - /var/log/june/cli-tools:/logs
    extra_hosts:
    - host.docker.internal:host-gateway
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids:
            - '0'
            capabilities:
            - gpu
    networks:
    - june_network
    profiles:
    - tools
    command:
    - bash
    - -c
    - 'echo ''CLI Tools container ready. Use: docker exec -it june-cli-tools bash''
      && tail -f /dev/null'
  model-tools:
    build:
      context: .
      dockerfile: ./Dockerfile.model-tools
    container_name: june-model-tools
    environment:
    - MODEL_CACHE_DIR=/models
    - HF_HOME=/models
    - TRANSFORMERS_CACHE=/models
    - HF_HUB_CACHE=/models
    - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    - CUDA_VISIBLE_DEVICES=0
    volumes:
    - ${MODEL_CACHE_DIR:-/home/rlee/models}:/models
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}:/data
    - ./:/workspace
    - /var/log/june/model-tools:/logs
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids:
            - '0'
            capabilities:
            - gpu
    networks:
    - june_network
    profiles:
    - tools
    command:
    - bash
    - -c
    - 'echo ''Model Tools container ready. Use: docker exec -it june-model-tools bash''
      && echo ''Available tools: python3, whisper, huggingface-cli''
      && tail -f /dev/null'
  # NOTE: inference-api service is being replaced by TensorRT-LLM (in home_infra/shared-network)
  # This service will be removed once TensorRT-LLM is set up and verified working.
  # For now, kept for backward compatibility during migration.
  #
  # To verify TensorRT-LLM is ready to replace this service:
  #   poetry run python -m essence verify-tensorrt-llm
  # Once verification passes, this service can be safely removed from docker-compose.yml.
  inference-api:
    build:
      context: .
      dockerfile: ./services/inference-api/Dockerfile
    container_name: june-inference-api
    restart: unless-stopped
    profiles:
      - legacy  # Use profile to disable by default - set LLM_URL=grpc://inference-api:50051 to use
    ports:
    - 50051:50051
    environment:
    - MODEL_NAME=${MODEL_NAME:-Qwen/Qwen3-30B-A3B-Thinking-2507}
    - MODEL_DEVICE=${MODEL_DEVICE:-cuda:0}
    - MAX_CONTEXT_LENGTH=${MAX_CONTEXT_LENGTH:-131072}
    - USE_YARN=${USE_YARN:-true}
    - USE_QUANTIZATION=${USE_QUANTIZATION:-true}
    - QUANTIZATION_BITS=${QUANTIZATION_BITS:-8}
    - MODEL_TEMPERATURE=${MODEL_TEMPERATURE:-0.7}
    - MODEL_MAX_TOKENS=${MODEL_MAX_TOKENS:-2048}
    - MODEL_TOP_P=${MODEL_TOP_P:-0.9}
    - MODEL_TOP_K=${MODEL_TOP_K:-}
    - MODEL_REPETITION_PENALTY=${MODEL_REPETITION_PENALTY:-}
    - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    - MODEL_CACHE_DIR=/models
    - HUGGINGFACE_CACHE_DIR=/models/huggingface
    - TRANSFORMERS_CACHE_DIR=/models/transformers
    - CUDA_VISIBLE_DEVICES=0
    - CUDA_MPS_ENABLE_PER_CTX_SM_PARTITIONING=1
    - ENABLE_TRACING=${ENABLE_TRACING:-true}
    - JAEGER_ENDPOINT=http://common-jaeger:14268/api/traces
    - JAEGER_AGENT_HOST=common-jaeger
    - JAEGER_AGENT_PORT=6831
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids:
            - '0'
            capabilities:
            - gpu
    volumes:
    - ${MODEL_CACHE_DIR:-/home/rlee/models}:/models
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/model_artifacts/inference-api:/app/model_artifacts
    - /var/log/june/inference-api:/logs
    networks:
    - june_network
    - shared-network
  stt:
    build:
      context: .
      dockerfile: ./services/stt/Dockerfile
    restart: unless-stopped
    ports:
    - 50052:50052
    environment:
    - MODEL_NAME=${STT_MODEL:-openai/whisper-large-v3}
    - MODEL_DEVICE=${STT_DEVICE:-cuda:0}
    - MODEL_CACHE_DIR=/models
    - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    - CUDA_VISIBLE_DEVICES=0
    - CUDA_MPS_ENABLE_PER_CTX_SM_PARTITIONING=1
    - ENABLE_TRACING=${ENABLE_TRACING:-true}
    - JAEGER_ENDPOINT=http://common-jaeger:14268/api/traces
    - JAEGER_AGENT_HOST=common-jaeger
    - JAEGER_AGENT_PORT=6831
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids:
            - '0'
            capabilities:
            - gpu
    volumes:
    - ${MODEL_CACHE_DIR:-/home/rlee/models}:/models
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/model_artifacts/stt:/app/model_artifacts
    - /var/log/june/stt:/logs
    healthcheck:
      test:
      - CMD
      - python
      - -c
      - import grpc; channel = grpc.insecure_channel('localhost:50052'); grpc.channel_ready_future(channel).result(timeout=5)
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
    - june_network
    - shared-network
  tts:
    build:
      context: .
      dockerfile: ./services/tts/Dockerfile
    restart: unless-stopped
    ports:
    - 50053:50053
    environment:
    - MODEL_NAME=${TTS_MODEL:-facebook/fastspeech2-en-ljspeech}
    - MODEL_DEVICE=${TTS_DEVICE:-cuda:0}
    - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    - CUDA_VISIBLE_DEVICES=0
    - CUDA_MPS_ENABLE_PER_CTX_SM_PARTITIONING=1
    - ENABLE_TRACING=${ENABLE_TRACING:-true}
    - JAEGER_ENDPOINT=http://common-jaeger:14268/api/traces
    - JAEGER_AGENT_HOST=common-jaeger
    - JAEGER_AGENT_PORT=6831
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids:
            - '0'
            capabilities:
            - gpu
    volumes:
    - ${MODEL_CACHE_DIR:-/home/rlee/models}:/models
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/model_artifacts/tts:/app/model_artifacts
    - ./services/tts:/app/services/tts
    - /var/log/june/tts:/logs
    healthcheck:
      test:
      - CMD
      - python
      - -c
      - import grpc; channel = grpc.insecure_channel('localhost:50053'); grpc.channel_ready_future(channel).result(timeout=5)
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
    - june_network
    - shared-network
  telegram:
    build:
      context: .
      dockerfile: ./services/telegram/Dockerfile
    restart: unless-stopped
    ports:
    - 8080:8080
    - 8443:8443
    environment:
    - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
    - TELEGRAM_AUTHORIZED_USERS=${TELEGRAM_AUTHORIZED_USERS:-39833618}
    - TELEGRAM_OWNER_USERS=${TELEGRAM_OWNER_USERS:-}
      # Owner users (personal accounts for direct communication - subset of whitelisted)
      # Example: TELEGRAM_OWNER_USERS=39833618
    - TELEGRAM_WHITELISTED_USERS=${TELEGRAM_WHITELISTED_USERS:-}
      # Whitelisted users (includes owners + other whitelisted users)
      # Example: TELEGRAM_WHITELISTED_USERS=39833618,987654321
    - CURSOR_AGENT_EXE=/usr/local/bin/cursor-tools/cursor-agent
    - CURSOR_API_KEY=${CURSOR_API_KEY:-}
    - CURSOR_AGENT=1
    - PATH=/usr/local/bin/cursor-tools:${PATH}
    - AGENTICNESS_STATE_DIR=/app/agenticness-state
    - NODE_ENV=development
    - TERM=dumb
    - HOSTNAME=${HOSTNAME:-telegram}
    - TELEGRAM_SERVICE_PORT=8080
    - TELEGRAM_USE_WEBHOOK=${TELEGRAM_USE_WEBHOOK:-false}
    - TELEGRAM_WEBHOOK_URL=${TELEGRAM_WEBHOOK_URL:-}
    - TELEGRAM_WEBHOOK_PORT=8443
    - TELEGRAM_WEBHOOK_PATH=/webhook
    - TELEGRAM_REQUEST_TIMEOUT=${TELEGRAM_REQUEST_TIMEOUT:-30.0}
    - TELEGRAM_RATE_LIMIT_PER_MINUTE=${TELEGRAM_RATE_LIMIT_PER_MINUTE:-10}
    - TELEGRAM_RATE_LIMIT_PER_HOUR=${TELEGRAM_RATE_LIMIT_PER_HOUR:-100}
    - TELEGRAM_RATE_LIMIT_PER_DAY=${TELEGRAM_RATE_LIMIT_PER_DAY:-500}
    - GRPC_MAX_CONNECTIONS_PER_SERVICE=${GRPC_MAX_CONNECTIONS_PER_SERVICE:-10}
    - GRPC_KEEPALIVE_TIME_MS=${GRPC_KEEPALIVE_TIME_MS:-30000}
    - GRPC_KEEPALIVE_TIMEOUT_MS=${GRPC_KEEPALIVE_TIMEOUT_MS:-5000}
    - LOG_LEVEL=${LOG_LEVEL:-INFO}
    - STT_URL=grpc://stt:50052
    - TTS_URL=grpc://tts:50053
    - LLM_URL=http://nim-qwen3:8000
      # LLM options: tensorrt-llm:8000 (default), nim-qwen3:8001 (NIM, requires NGC_API_KEY in home_infra), inference-api:50051 (legacy)
    - ENABLE_TRACING=${ENABLE_TRACING:-true}
    - TODO_SERVICE_API_KEY=${TODO_SERVICE_API_KEY:-}
      # API key for Todorama service (required for creating tasks)
    - TODORAMA_API_KEY=${TODORAMA_API_KEY:-}
      # Alternative name for Todorama API key
    - JAEGER_ENDPOINT=http://common-jaeger:14268/api/traces
    - JAEGER_AGENT_HOST=common-jaeger
    - JAEGER_AGENT_PORT=6831
    depends_on:
    - stt
    - tts
    # Note: TensorRT-LLM is in home_infra (shared-network), not june docker-compose.yml
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8080/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
    - ${AGENTICNESS_DIR:-/home/rlee/dev/agenticness}:/app/agenticness:ro
    - ${HOME}/.local/share/cursor-agent/versions/2025.11.06-8fe8a63:/usr/local/bin/cursor-tools:ro
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/agenticness-state:/app/agenticness-state
    - ${HOME}/.config/cursor/auth.json:/home/june/.config/cursor/auth.json:ro
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/cursor-container-config:/home/june/.config/cursor:rw
    - ./config/cursor-mcp-config.json:/home/june/.config/cursor/mcp.json:ro
    - ${HOME}/.cursor:/home/june/.cursor:rw
    - /var/log/june/telegram:/logs
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/var-data:/var/data
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
    - june_network
    - shared-network
  discord:
    build:
      context: .
      dockerfile: ./services/discord/Dockerfile
    restart: unless-stopped
    ports:
    - 8081:8081
    environment:
    - DISCORD_BOT_TOKEN=${DISCORD_BOT_TOKEN:-}
    - DISCORD_AUTHORIZED_USERS=${DISCORD_AUTHORIZED_USERS:-}
    - DISCORD_OWNER_USERS=${DISCORD_OWNER_USERS:-}
      # Owner users (personal accounts for direct communication - subset of whitelisted)
      # Example: DISCORD_OWNER_USERS=123456789012345678
    - DISCORD_WHITELISTED_USERS=${DISCORD_WHITELISTED_USERS:-}
      # Whitelisted users (includes owners + other whitelisted users)
      # Example: DISCORD_WHITELISTED_USERS=123456789012345678,987654321098765432
    - CURSOR_AGENT_EXE=/usr/local/bin/cursor-tools/cursor-agent
    - CURSOR_API_KEY=${CURSOR_API_KEY:-}
    - CURSOR_AGENT=1
    - PATH=/usr/local/bin/cursor-tools:${PATH}
    - AGENTICNESS_STATE_DIR=/app/agenticness-state
    - NODE_ENV=development
    - TERM=dumb
    - HOSTNAME=${HOSTNAME:-discord}
    - DISCORD_SERVICE_PORT=8081
    - GRPC_MAX_CONNECTIONS_PER_SERVICE=${GRPC_MAX_CONNECTIONS_PER_SERVICE:-10}
    - GRPC_KEEPALIVE_TIME_MS=${GRPC_KEEPALIVE_TIME_MS:-30000}
    - GRPC_KEEPALIVE_TIMEOUT_MS=${GRPC_KEEPALIVE_TIMEOUT_MS:-5000}
    - LOG_LEVEL=${LOG_LEVEL:-INFO}
    - STT_URL=grpc://stt:50052
    - TTS_URL=grpc://tts:50053
    - LLM_URL=http://nim-qwen3:8000
      # LLM options: tensorrt-llm:8000 (default), nim-qwen3:8001 (NIM, requires NGC_API_KEY in home_infra), inference-api:50051 (legacy)
    - ENABLE_TRACING=${ENABLE_TRACING:-true}
    - TODO_SERVICE_API_KEY=${TODO_SERVICE_API_KEY:-}
      # API key for Todorama service (required for creating tasks)
    - TODORAMA_API_KEY=${TODORAMA_API_KEY:-}
      # Alternative name for Todorama API key
    - JAEGER_ENDPOINT=http://common-jaeger:14268/api/traces
    - JAEGER_AGENT_HOST=common-jaeger
    - JAEGER_AGENT_PORT=6831
    depends_on:
    - stt
    - tts
    # Note: TensorRT-LLM is in home_infra (shared-network), not june docker-compose.yml
    volumes:
    - ${AGENTICNESS_DIR:-/home/rlee/dev/agenticness}:/app/agenticness:ro
    - ${HOME}/.local/share/cursor-agent/versions/2025.11.06-8fe8a63:/usr/local/bin/cursor-tools:ro
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/agenticness-state:/app/agenticness-state
    - ${HOME}/.config/cursor/auth.json:/home/june/.config/cursor/auth.json:ro
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/cursor-container-config:/home/june/.config/cursor:rw
    - ./config/cursor-mcp-config.json:/home/june/.config/cursor/mcp.json:ro
    - ${HOME}/.cursor:/home/june/.cursor:rw
    - /var/log/june/discord:/logs
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/var-data:/var/data
    networks:
    - june_network
    - shared-network
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8081/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  message-api:
    build:
      context: .
      dockerfile: ./services/message-api/Dockerfile
    container_name: june-message-api
    restart: unless-stopped
    ports:
    - 8083:8082
    environment:
    - MESSAGE_API_PORT=8082
    - MESSAGE_API_HOST=0.0.0.0
    - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
    - DISCORD_BOT_TOKEN=${DISCORD_BOT_TOKEN}
    - TELEGRAM_OWNER_USERS=${TELEGRAM_OWNER_USERS:-}
    - TELEGRAM_WHITELISTED_USERS=${TELEGRAM_WHITELISTED_USERS:-}
    - DISCORD_OWNER_USERS=${DISCORD_OWNER_USERS:-}
    - DISCORD_WHITELISTED_USERS=${DISCORD_WHITELISTED_USERS:-}
    - LOG_LEVEL=${LOG_LEVEL:-INFO}
    - ENABLE_TRACING=${ENABLE_TRACING:-true}
    - JAEGER_ENDPOINT=http://common-jaeger:14268/api/traces
    - JAEGER_AGENT_HOST=common-jaeger
    - JAEGER_AGENT_PORT=6831
    volumes:
    - /var/log/june/message-api:/logs
    - ${JUNE_DATA_DIR:-/home/rlee/june_data}/var-data:/var/data
    networks:
    - june_network
    - shared-network
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8082/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  integration-test:
    build:
      context: .
      dockerfile: ./services/integration-test/Dockerfile
    container_name: june-integration-test
    restart: unless-stopped
    ports:
    - 8084:8082
    environment:
    - INTEGRATION_TEST_SERVICE_PORT=8082
    - INTEGRATION_TEST_SERVICE_HOST=0.0.0.0
    - LOG_LEVEL=${LOG_LEVEL:-INFO}
    - ENABLE_TRACING=${ENABLE_TRACING:-true}
    - JAEGER_ENDPOINT=http://common-jaeger:14268/api/traces
    - JAEGER_AGENT_HOST=common-jaeger
    - JAEGER_AGENT_PORT=6831
    volumes:
    - /var/log/june/integration-test:/logs
    networks:
    - june_network
    - shared-network
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8082/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
networks:
  june_network:
    driver: bridge
  shared-network:
    external: true
    name: shared_network
