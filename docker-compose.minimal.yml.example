# Minimal docker-compose.yml for Telegram/Discord voice round trip
# This is the bare essentials - 4 core services (STT, TTS, Telegram, Discord)
# LLM inference is provided by TensorRT-LLM container in home_infra (shared-network)
# Legacy inference-api service available via --profile legacy if needed

services:
  # STT Service - Speech-to-Text
  stt:
    build:
      context: .
      dockerfile: ./services/stt/Dockerfile
    restart: unless-stopped
    ports:
      - 50052:50052
    environment:
      - MODEL_NAME=${STT_MODEL:-openai/whisper-large-v3}
      - MODEL_DEVICE=${STT_DEVICE:-cuda:0}
      - MODEL_CACHE_DIR=/models
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    volumes:
      - ${MODEL_CACHE_DIR:-/home/rlee/models}:/models
      - ${JUNE_DATA_DIR:-/home/rlee/june_data}/model_artifacts/stt:/app/model_artifacts
      - /var/log/june/stt:/logs
    networks:
      - june_network
      - shared-network  # For TensorRT-LLM, Jaeger, Prometheus (from home_infra)
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; grpc.insecure_channel('localhost:50052').check_connectivity_state(True)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # TTS Service - Text-to-Speech
  tts:
    build:
      context: .
      dockerfile: ./services/tts/Dockerfile
    restart: unless-stopped
    ports:
      - 50053:50053
    environment:
      - MODEL_NAME=${TTS_MODEL:-facebook/fastspeech2-en-ljspeech}
      - MODEL_DEVICE=${TTS_DEVICE:-cuda:0}
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    volumes:
      - ${MODEL_CACHE_DIR:-/home/rlee/models}:/models
      - ${JUNE_DATA_DIR:-/home/rlee/june_data}/model_artifacts/tts:/app/model_artifacts
      - ./services/tts:/app
      - /var/log/june/tts:/logs
    networks:
      - june_network
      - shared-network  # For TensorRT-LLM, Jaeger, Prometheus (from home_infra)
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; grpc.insecure_channel('localhost:50053').check_connectivity_state(True)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # NOTE: LLM Inference is provided by TensorRT-LLM container in home_infra
  # TensorRT-LLM is accessible via shared-network as tensorrt-llm:8000
  # See REFACTOR_PLAN.md Phase 15 for TensorRT-LLM setup instructions
  # Legacy inference-api service available via: docker compose --profile legacy up -d inference-api

  # Telegram Bot Service - Orchestrates the pipeline
  telegram:
    build:
      context: .
      dockerfile: ./services/telegram/Dockerfile
    restart: unless-stopped
    ports:
      - 8080:8080
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_SERVICE_PORT=8080
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - STT_URL=grpc://stt:50052
      - TTS_URL=grpc://tts:50053
      - LLM_URL=grpc://tensorrt-llm:8000  # TensorRT-LLM in home_infra/shared-network
      # Legacy: Use inference-api with --profile legacy: LLM_URL=grpc://inference-api:50051
    depends_on:
      - stt
      - tts
      # TensorRT-LLM is in home_infra/shared-network, not in this compose file
    volumes:
      - /var/log/june/telegram:/logs
    networks:
      - june_network
      - shared-network  # For TensorRT-LLM, Jaeger, Prometheus (from home_infra)
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Discord Bot Service - Orchestrates the pipeline (shares code with telegram)
  discord:
    build:
      context: .
      dockerfile: ./services/discord/Dockerfile
    restart: unless-stopped
    ports:
      - 8081:8081
    environment:
      - DISCORD_BOT_TOKEN=${DISCORD_BOT_TOKEN}
      - DISCORD_SERVICE_PORT=8081
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - STT_URL=grpc://stt:50052
      - TTS_URL=grpc://tts:50053
      - LLM_URL=grpc://tensorrt-llm:8000  # TensorRT-LLM in home_infra/shared-network
      # Legacy: Use inference-api with --profile legacy: LLM_URL=grpc://inference-api:50051
    depends_on:
      - stt
      - tts
      # TensorRT-LLM is in home_infra/shared-network, not in this compose file
    volumes:
      - /var/log/june/discord:/logs
    networks:
      - june_network
      - shared-network  # For TensorRT-LLM, Jaeger, Prometheus (from home_infra)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

networks:
  june_network:
    driver: bridge
  shared-network:
    external: true
    name: shared_network  # Connects to home_infra for TensorRT-LLM, Jaeger, Prometheus, etc.

# That's it! Only 4 core services needed (STT, TTS, Telegram, Discord).
# LLM inference provided by TensorRT-LLM container in home_infra (shared-network).
# Telegram and Discord share code via essence/chat/ module.
# No PostgreSQL, Redis, MinIO, NATS, Gateway, Webapp, etc.
# Legacy inference-api service available via --profile legacy if needed.

