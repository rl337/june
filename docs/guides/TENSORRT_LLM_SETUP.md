# TensorRT-LLM Setup Guide

This guide documents the migration from the legacy `inference-api` service to TensorRT-LLM for optimized GPU inference in the June project.

## Overview

**Status:** Phase 15 - Migration in progress

The June project is migrating from the legacy `inference-api` service to **TensorRT-LLM** running in a Triton Inference Server container. TensorRT-LLM provides optimized GPU inference with better performance and memory efficiency for large language models like Qwen3-30B-A3B-Thinking-2507.

### Architecture

- **Legacy:** `inference-api` service (disabled by default, available via `--profile legacy`)
- **Current:** TensorRT-LLM container in `home_infra` (accessible via `shared-network` as `tensorrt-llm:8000`)
- **Migration Status:** Code migration complete ✅, infrastructure setup complete ✅, model compilation pending ⏳

## Prerequisites

- NVIDIA GPU with 20GB+ VRAM (for Qwen3-30B with 8-bit quantization)
- NVIDIA Container Toolkit installed and configured
- Docker with GPU support enabled
- Access to `home_infra` project (TensorRT-LLM container is deployed there)
- Model cache directory: `/home/rlee/models`

## Infrastructure Setup

### TensorRT-LLM Container (in home_infra)

The TensorRT-LLM container is configured in `/home/rlee/dev/home_infra/docker-compose.yml`:

```yaml
tensorrt-llm:
  image: nvcr.io/nvidia/tritonserver:24.10-py3
  container_name: common-tensorrt-llm
  expose:
    - "8000"  # gRPC endpoint (internal to shared-network)
    - "8002"  # HTTP endpoint (internal, for health checks)
  environment:
    - TRITON_MODEL_REPOSITORY=/models/triton-repository
    - CUDA_VISIBLE_DEVICES=0
    - MODEL_NAME=Qwen/Qwen3-30B-A3B-Thinking-2507
    - QUANTIZATION_BITS=8
    - MAX_CONTEXT_LENGTH=131072
  volumes:
    - /home/rlee/models:/models
  networks:
    - shared-network
  command:
    - tritonserver
    - --model-repository=/models/triton-repository
    - --allow-gpu-metrics=true
    - --allow-http=true
    - --http-port=8002
    - --grpc-port=8000
```

**To start the container:**
```bash
cd /home/rlee/dev/home_infra
docker compose up -d tensorrt-llm
```

**To check status:**
```bash
docker compose ps tensorrt-llm
docker compose logs tensorrt-llm
```

## Model Repository Setup

### Creating Model Repository Structure

Use the `setup-triton-repository` command to create the directory structure:

```bash
# Create repository structure for a model
poetry run -m essence setup-triton-repository --action create --model qwen3-30b --version 1

# Validate the structure
poetry run -m essence setup-triton-repository --action validate --model qwen3-30b

# List all models in repository
poetry run -m essence setup-triton-repository --action list
```

This creates the directory structure at `/home/rlee/models/triton-repository/<model_name>/<version>/` and includes a README.md with instructions.

### Required Files

Each model directory must contain:

1. **config.pbtxt** - Triton model configuration file
   - Defines model inputs/outputs, instance groups, and TensorRT-LLM backend settings
   - Example structure:
     ```protobuf
     name: "qwen3-30b"
     platform: "tensorrt_llm"
     max_batch_size: 0
     input [
       {
         name: "text_input"
         data_type: TYPE_STRING
         dims: [ -1 ]
       }
     ]
     output [
       {
         name: "text_output"
         data_type: TYPE_STRING
         dims: [ -1 ]
       }
     ]
     instance_group [
       {
         count: 1
         kind: KIND_GPU
       }
     ]
     ```

2. **TensorRT-LLM Engine Files** - Compiled model engine files
   - Typically includes: `*.engine` files, weights, and other artifacts
   - Generated by TensorRT-LLM build tools

3. **Tokenizer Files** - Tokenizer configuration and vocabulary
   - `tokenizer.json` or `tokenizer_config.json`
   - `vocab.json` or similar vocabulary files
   - Usually copied from HuggingFace model directory

## Model Compilation

**Status:** ⏳ TODO - Requires TensorRT-LLM build tools

Models must be compiled using TensorRT-LLM build tools before they can be loaded. Use the `compile-model` command to validate prerequisites and get compilation guidance.

### Prerequisites Check

Before compiling, validate your environment:

```bash
# Check prerequisites and get compilation guidance
poetry run -m essence compile-model --model qwen3-30b --check-prerequisites --generate-template
```

This command checks:
- ✅ GPU availability (nvidia-smi)
- ✅ Model repository structure
- ✅ TensorRT-LLM build tools availability
- ✅ Whether model is already compiled

It also generates compilation command templates with proper options.

### Compilation Process

1. **Download the model** (if not already downloaded):
   ```bash
   docker compose run --rm cli-tools \
     poetry run -m essence download-models --model Qwen/Qwen3-30B-A3B-Thinking-2507
   ```

2. **Validate prerequisites**:
   ```bash
   poetry run -m essence compile-model --model qwen3-30b --check-prerequisites
   ```

3. **Get compilation template**:
   ```bash
   poetry run -m essence compile-model --model qwen3-30b --generate-template
   ```

4. **Compile the model** using TensorRT-LLM build tools:
   - Use the command template from step 3
   - Configure quantization (8-bit as specified in environment variables)
   - Set max context length (131072 tokens)
   - Generate TensorRT-LLM engine files
   - **Note:** This step requires TensorRT-LLM build tools and must be done manually or via external scripts

5. **Place compiled files** in the model repository:
   - Copy engine files to `/home/rlee/models/triton-repository/<model_name>/<version>/`
   - Create `config.pbtxt` with model configuration
   - Copy tokenizer files from HuggingFace model directory

6. **Validate compiled model**:
   ```bash
   poetry run -m essence setup-triton-repository --action validate --model qwen3-30b
   ```

## Model Management

### Loading Models

Once models are compiled and placed in the repository, use the `manage-tensorrt-llm` command to load them:

```bash
# Load a model
poetry run -m essence manage-tensorrt-llm --action load --model qwen3-30b

# Check model status
poetry run -m essence manage-tensorrt-llm --action status --model qwen3-30b

# List all models
poetry run -m essence manage-tensorrt-llm --action list

# Unload a model
poetry run -m essence manage-tensorrt-llm --action unload --model qwen3-30b
```

### Model Switching

To switch between models:

1. Unload the current model:
   ```bash
   poetry run -m essence manage-tensorrt-llm --action unload --model <current-model>
   ```

2. Load the new model:
   ```bash
   poetry run -m essence manage-tensorrt-llm --action load --model <new-model>
   ```

**Note:** Only one model can be loaded at a time. Unload the current model before loading a new one.

## Verification

### Comprehensive Migration Verification

Use the `verify-tensorrt-llm` command to perform comprehensive checks and determine migration readiness:

```bash
# Run full verification
poetry run -m essence verify-tensorrt-llm

# With custom URLs
poetry run -m essence verify-tensorrt-llm \
  --tensorrt-llm-url http://tensorrt-llm:8002 \
  --grpc-host tensorrt-llm \
  --grpc-port 8000

# JSON output for automation/CI
poetry run -m essence verify-tensorrt-llm --json
```

This command checks:
- ✅ Container HTTP API connectivity (TensorRT-LLM HTTP endpoint)
- ✅ gRPC endpoint connectivity (tensorrt-llm:8000)
- ✅ Model repository structure and accessibility
- ✅ GPU availability (nvidia-smi)

**Exit codes:**
- `0` - Migration ready (all critical checks pass)
- `1` - Not ready (one or more critical checks failed)

Use this command before removing the legacy `inference-api` service from `docker-compose.yml`.

### Check TensorRT-LLM Container Status

```bash
# Check if container is running
cd /home/rlee/dev/home_infra
docker compose ps tensorrt-llm

# Check container logs
docker compose logs tensorrt-llm

# Check health endpoint (from within shared-network)
curl http://tensorrt-llm:8002/health
```

### Verify Model Loading

```bash
# Check model status
poetry run -m essence manage-tensorrt-llm --action status --model qwen3-30b

# List models in repository
poetry run -m essence manage-tensorrt-llm --action list

# Verify GPU usage (must use GPU, CPU fallback FORBIDDEN)
nvidia-smi
```

### Test Model Inference

Once a model is loaded, test inference via the gRPC interface:

```python
import grpc
from june_grpc_api import llm_pb2, llm_pb2_grpc

# Connect to TensorRT-LLM
channel = grpc.insecure_channel("tensorrt-llm:8000")
stub = llm_pb2_grpc.LLMInferenceStub(channel)

# Test generation
request = llm_pb2.GenerationRequest(
    prompt="Hello, how are you?",
    params=llm_pb2.GenerationParameters(
        max_tokens=100,
        temperature=0.7
    )
)
response = stub.Generate(request)
print(response.text)
```

## June Service Configuration

### Default Configuration

All June services (telegram, discord, coding-agent, benchmarks) are configured to use TensorRT-LLM by default:

- **LLM URL:** `grpc://tensorrt-llm:8000` (internal to shared-network)
- **Environment Variable:** `LLM_URL` or `INFERENCE_API_URL` (backward compatible)

### Backward Compatibility

The legacy `inference-api` service is still available for backward compatibility:

```bash
# Start legacy inference-api service
docker compose --profile legacy up -d inference-api

# Use legacy service (set environment variable)
export LLM_URL=grpc://inference-api:50051
```

## Troubleshooting

### Container Not Starting

1. **Check GPU availability:**
   ```bash
   nvidia-smi
   docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
   ```

2. **Check NVIDIA Container Toolkit:**
   ```bash
   docker info | grep -i runtime
   # Should show: nvidia
   ```

3. **Check container logs:**
   ```bash
   cd /home/rlee/dev/home_infra
   docker compose logs tensorrt-llm
   ```

### Model Not Loading

1. **Verify model repository structure:**
   ```bash
   poetry run -m essence setup-triton-repository --action validate --model <model-name>
   ```

2. **Check required files:**
   - `config.pbtxt` exists
   - Engine files (`.engine`) exist
   - Tokenizer files exist

3. **Check Triton logs:**
   ```bash
   docker compose logs tensorrt-llm | grep -i error
   ```

### Connection Issues

1. **Verify network connectivity:**
   ```bash
   # From june container, test connection to TensorRT-LLM
   docker compose exec telegram curl http://tensorrt-llm:8002/health
   ```

2. **Check shared-network:**
   ```bash
   docker network inspect shared_network | grep -A 5 tensorrt-llm
   ```

### GPU Not Being Used

**CRITICAL:** Large models (30B+) must NEVER load on CPU. If GPU is not available, TensorRT-LLM should fail to start, not attempt CPU loading.

1. **Verify GPU access:**
   ```bash
   docker compose exec tensorrt-llm nvidia-smi
   ```

2. **Check CUDA_VISIBLE_DEVICES:**
   ```bash
   docker compose exec tensorrt-llm env | grep CUDA
   ```

3. **Verify GPU in config.pbtxt:**
   - Ensure `instance_group` has `kind: KIND_GPU`
   - Not `kind: KIND_CPU`

## Migration Checklist

- [x] TensorRT-LLM container configured in home_infra
- [x] Model loading/unloading API implemented
- [x] Model repository setup command created
- [x] June services updated to use TensorRT-LLM by default
- [x] Documentation updated (AGENTS.md, README.md)
- [ ] Model compilation process documented/automated
- [ ] Qwen3-30B-A3B-Thinking-2507 compiled and loaded
- [ ] GPU usage verified
- [ ] Inference tested end-to-end
- [ ] Legacy inference-api service removed from docker-compose.yml

## Related Commands

- `poetry run -m essence setup-triton-repository` - Manage model repository structure
- `poetry run -m essence manage-tensorrt-llm` - Load/unload models
- `poetry run -m essence download-models` - Download models from HuggingFace
- `poetry run -m essence check-environment` - Pre-flight environment validation

## Related Documentation

- [REFACTOR_PLAN.md](../../REFACTOR_PLAN.md) - Phase 15: TensorRT-LLM Integration
- [AGENTS.md](AGENTS.md) - Agent development guidelines
- [README.md](../../README.md) - Project overview and quick start

## References

- [TensorRT-LLM Documentation](https://nvidia.github.io/TensorRT-LLM/)
- [Triton Inference Server Documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
