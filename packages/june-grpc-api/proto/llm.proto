syntax = "proto3";

package june.llm;

// LLM Inference Service
service LLMInference {
  // Streaming generation
  rpc GenerateStream(GenerationRequest) returns (stream GenerationChunk);
  
  // One-shot generation
  rpc Generate(GenerationRequest) returns (GenerationResponse);
  
  // Streaming chat
  rpc ChatStream(ChatRequest) returns (stream ChatChunk);
  
  // One-shot chat
  rpc Chat(ChatRequest) returns (ChatResponse);
  
  // Embedding generation
  rpc Embed(EmbeddingRequest) returns (EmbeddingResponse);
  
  // Health check
  rpc HealthCheck(HealthRequest) returns (HealthResponse);
}

// Generation request
message GenerationRequest {
  string prompt = 1;
  GenerationParameters params = 2;
  Context context = 3;
  bool stream = 4;
}

// Chat request with conversation history
message ChatRequest {
  repeated ChatMessage messages = 1;
  GenerationParameters params = 2;
  Context context = 3;
  bool stream = 4;
}

// Chat message
message ChatMessage {
  string role = 1;  // "system", "user", "assistant", "tool"
  string content = 2;
  string name = 3;  // For function calls or multi-party
  FunctionCall function_call = 4;
  repeated ToolCall tool_calls = 5;
}

// Function call (for function calling)
message FunctionCall {
  string name = 1;
  string arguments = 2;  // JSON string
}

// Tool call (for parallel tools)
message ToolCall {
  string id = 1;
  string type = 2;  // "function"
  FunctionCall function = 3;
}

// Generation parameters
message GenerationParameters {
  int32 max_tokens = 1;
  float temperature = 2;
  float top_p = 3;
  float top_k = 4;
  float repetition_penalty = 5;
  repeated string stop_sequences = 6;
  int32 seed = 7;
}

// Context information
message Context {
  string user_id = 1;
  string session_id = 2;
  repeated string rag_document_ids = 3;  // For RAG retrieval
  bool enable_tools = 4;
  repeated ToolDefinition available_tools = 5;
  int32 max_context_tokens = 6;  // Effective context window size
}

// Tool definition
message ToolDefinition {
  string name = 1;
  string description = 2;
  string parameters_schema = 3;  // JSON schema
}

// Streaming generation chunk
message GenerationChunk {
  string token = 1;
  bool is_final = 2;
  int32 index = 3;
  float logprob = 4;
  repeated ToolCall tool_calls = 5;
  FinishReason finish_reason = 6;
}

// Chat chunk (similar but with role)
message ChatChunk {
  string content_delta = 1;
  string role = 2;
  bool is_final = 3;
  repeated ToolCall tool_calls = 4;
  FinishReason finish_reason = 5;
}

// Complete generation response
message GenerationResponse {
  string text = 1;
  int32 tokens_generated = 2;
  float tokens_per_second = 3;
  repeated ToolCall tool_calls = 4;
  FinishReason finish_reason = 5;
  UsageStats usage = 6;
}

// Chat response
message ChatResponse {
  ChatMessage message = 1;
  int32 tokens_generated = 2;
  float tokens_per_second = 3;
  UsageStats usage = 4;
}

// Finish reason
enum FinishReason {
  STOP = 0;
  LENGTH = 1;
  TOOL_CALLS = 2;
  ERROR = 3;
}

// Usage statistics
message UsageStats {
  int32 prompt_tokens = 1;
  int32 completion_tokens = 2;
  int32 total_tokens = 3;
  int32 prompt_cache_hits = 4;
}

// Embedding request
message EmbeddingRequest {
  repeated string texts = 1;
  string model = 2;
}

// Embedding response
message EmbeddingResponse {
  repeated float embeddings = 1;  // Flattened: [text1_dim1, text1_dim2, ..., textN_dimD]
  int32 dimension = 2;
}

// Health check messages
message HealthRequest {}

message HealthResponse {
  bool healthy = 1;
  string version = 2;
  string model_name = 3;
  int32 max_context_length = 4;
  bool supports_streaming = 5;
  repeated string available_tools = 6;
}



