syntax = "proto3";

package june.asr;

// Speech Recognition Service
service SpeechToText {
  // Streaming speech recognition
  rpc RecognizeStream(stream AudioChunk) returns (stream RecognitionResult);
  
  // One-shot recognition
  rpc Recognize(RecognitionRequest) returns (RecognitionResponse);
  
  // Health check
  rpc HealthCheck(HealthRequest) returns (HealthResponse);
}

// Audio chunk for streaming
message AudioChunk {
  bytes audio_data = 1;
  int32 sample_rate = 2;
  int32 channels = 3;
  string encoding = 4;  // "pcm", "opus", "webm"
  int64 timestamp_us = 5;
}

// Recognition request for one-shot
message RecognitionRequest {
  bytes audio_data = 1;
  int32 sample_rate = 2;
  string encoding = 3;
  RecognitionConfig config = 4;
}

// Recognition configuration
message RecognitionConfig {
  string language = 1;  // ISO 639-1 code, e.g. "en"
  bool interim_results = 2;
  bool enable_vad = 3;  // Voice Activity Detection
  bool enable_diarization = 4;
  bool enable_timestamps = 5;
}

// Partial or final recognition result
message RecognitionResult {
  string transcript = 1;
  bool is_final = 2;
  float confidence = 3;
  repeated WordInfo words = 4;
  int64 start_time_us = 5;
  int64 end_time_us = 6;
  string speaker_id = 7;  // If diarization enabled
  string detected_language = 8;  // ISO 639-1 code of detected language (when auto-detection is used)
}

// Word-level information
message WordInfo {
  string word = 1;
  float confidence = 2;
  int64 start_time_us = 3;
  int64 end_time_us = 4;
}

// Complete recognition response
message RecognitionResponse {
  repeated RecognitionResult results = 1;
  int32 processing_time_ms = 2;
}

// Health check messages
message HealthRequest {}

message HealthResponse {
  bool healthy = 1;
  string version = 2;
  string model_name = 3;
}






